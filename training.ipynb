{
 "cells": [
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# MODEL EXPORT FOR WEBSITE INTEGRATION  \n# =============================================================================\n\ndef save_model_for_website(model, model_name=\"piano_transcription_weights.pth\"):\n    \"\"\"\n    Save the trained model in a format suitable for the Flask website\n    \n    Args:\n        model: The trained CRNN_OnsetsAndFrames model\n        model_name: Name for the saved model file\n    \"\"\"\n    try:\n        # Save model state dict (recommended approach)\n        model_path = f\"/content/drive/MyDrive/APS360_Team_2_Project/models/{model_name}\"\n        torch.save(model.state_dict(), model_path)\n        print(f\"‚úÖ Model saved to: {model_path}\")\n        \n        # Also save to current directory for easy download\n        local_path = model_name\n        torch.save(model.state_dict(), local_path)\n        print(f\"‚úÖ Model also saved locally: {local_path}\")\n        \n        # Print model info\n        total_params = sum(p.numel() for p in model.parameters())\n        model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n        \n        print(f\"üìä Model Info:\")\n        print(f\"   Total parameters: {total_params:,}\")\n        print(f\"   Model size: {model_size_mb:.1f} MB\")\n        print(f\"   Architecture: {model.name}\")\n        \n        # Save model configuration as well\n        config = {\n            \"model_name\": model.name,\n            \"num_pitches\": model.num_pitches,\n            \"total_parameters\": total_params,\n            \"model_size_mb\": model_size_mb,\n            \"training_info\": {\n                \"sample_rate\": SAMPLE_RATE,\n                \"n_mels\": N_MELS,\n                \"window_size_seconds\": WINDOW_SIZE_SECONDS,\n                \"min_midi_note\": MIN_MIDI_NOTE,\n                \"max_midi_note\": MAX_MIDI_NOTE\n            }\n        }\n        \n        config_path = model_name.replace('.pth', '_config.json')\n        with open(config_path, 'w') as f:\n            json.dump(config, f, indent=2)\n        print(f\"‚úÖ Model config saved: {config_path}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to save model: {e}\")\n        return False\n\ndef download_model_from_colab():\n    \"\"\"\n    Instructions for downloading the model from Google Colab\n    \"\"\"\n    print(\"üì• To download the model to your local machine:\")\n    print(\"1. Run the save_model_for_website() function above\")\n    print(\"2. In Colab, go to Files tab on the left\")\n    print(\"3. Find 'piano_transcription_weights.pth' and download it\")\n    print(\"4. Place it in your website's backend/weights/ directory\")\n    print(\"5. Restart your Flask server to load the new model\")\n    print(\"\")\n    print(\"üí° Alternative: Use Google Drive sync\")\n    print(\"   The model is also saved to your Drive at:\")\n    print(\"   /content/drive/MyDrive/APS360_Team_2_Project/models/\")\n\n# Test model saving (run this after training)\nprint(\"üìù Model export functions ready!\")\nprint(\"After training completes, run:\")\nprint(\"   save_model_for_website(model)\")\nprint(\"   download_model_from_colab()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Model Export for Website Integration\n\nThis section handles saving the trained model for use in the Flask website.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84RYcsMMlxo1"
   },
   "source": [
    "# APS360 Piano Transcription Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dP_q2fjlxo5"
   },
   "source": [
    "## 1. Setup and Imports\n",
    "Every Import needed for the project is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCFtO0HSlxo5",
    "outputId": "8c006f26-d02b-4d7b-fafe-bd6257e8bd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Running in Google Colab\n",
      "Using CPU with 2 cores\n",
      "Workers: 0\n"
     ]
    }
   ],
   "source": [
    "# We setup the environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install librosa soundfile pretty_midi -q\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Standard imports\n",
    "import os, json, warnings, zipfile, urllib.request, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pretty_midi\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We set the device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    NUM_WORKERS = 2\n",
    "    PIN_MEMORY = True\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    NUM_WORKERS = min(4, os.cpu_count() // 2) if os.cpu_count() > 2 else 0\n",
    "    PIN_MEMORY = False\n",
    "    print(f\"Using CPU with {os.cpu_count()} cores\")\n",
    "\n",
    "print(f\"Workers: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EuOsOMYlxo7"
   },
   "source": [
    "## 2. Configuration and Parameters\n",
    "We can modify this if we need to change the data configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37ikMTN6lxo7",
    "outputId": "e947edea-ec1f-45c2-8784-79eb6baac9c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Audio: 16000Hz, 128 mel bins\n",
      "Piano: 88 keys (A0-C8)\n",
      "Window: 10.0s (312 frames)\n",
      "Batch size: 8\n",
      "Augmentation ratio: 50% mixed audio\n",
      "Caching: True\n",
      "Cache regen: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "# Piano parameters\n",
    "MIN_MIDI_NOTE = 21   # A0\n",
    "MAX_MIDI_NOTE = 108  # C8\n",
    "N_PIANO_KEYS = MAX_MIDI_NOTE - MIN_MIDI_NOTE + 1  # 88 keys\n",
    "MIDDLE_C = 60\n",
    "\n",
    "# Sliding window configuration\n",
    "WINDOW_SIZE_SECONDS = 10.0\n",
    "MIN_RECORDING_LENGTH = 5.0\n",
    "WINDOW_OVERLAP = 0.25\n",
    "WINDOW_SIZE_FRAMES = int(WINDOW_SIZE_SECONDS * SAMPLE_RATE / HOP_LENGTH)\n",
    "STRIDE_FRAMES = int(WINDOW_SIZE_FRAMES * (1 - WINDOW_OVERLAP))\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "RANDOM_SEED = 1000\n",
    "\n",
    "# Batch sizes based on device\n",
    "if DEVICE.type == 'cuda':\n",
    "    BATCH_SIZE = 16\n",
    "    PROCESSING_BATCH_SIZE = 4\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "    PROCESSING_BATCH_SIZE = 2\n",
    "\n",
    "# Dataset configuration\n",
    "DATASETS_TO_USE = {\n",
    "    'maestro': True,      # Clean piano performances\n",
    "    'musdb_augmentation': True  # Songs for mixed audio training\n",
    "}\n",
    "\n",
    "# Data augmentation parameters\n",
    "AUGMENTATION_RATIO = 0.5  # 50% will be augmented (clean + mixed)\n",
    "MIX_VOLUME_RANGE = (0.3, 0.8)  # Volume range for mixing piano with accompaniment\n",
    "\n",
    "# Caching\n",
    "CACHE_PROCESSED_DATA = True # if we want to use the cached data\n",
    "REGENERATE_CACHE = False # if we want to delete all the old cached data and re-cache\n",
    "CACHE_VERSION = \"v2.0\"\n",
    "CACHE_SUFFIX = f\"maestro_musdb_win{WINDOW_SIZE_SECONDS}s_mel{N_MELS}_{CACHE_VERSION}\"\n",
    "\n",
    "# Print the configuration\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Audio: {SAMPLE_RATE}Hz, {N_MELS} mel bins\")\n",
    "print(f\"Piano: {N_PIANO_KEYS} keys (A0-C8)\")\n",
    "print(f\"Window: {WINDOW_SIZE_SECONDS}s ({WINDOW_SIZE_FRAMES} frames)\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Augmentation ratio: {AUGMENTATION_RATIO:.0%} mixed audio\")\n",
    "print(f\"Caching: {CACHE_PROCESSED_DATA}\")\n",
    "print(f\"Cache regen: {REGENERATE_CACHE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOrtWXq4lxo8"
   },
   "source": [
    "## 3. Data Loading Functions\n",
    "All the functions here are for setting up the directory, downloading the raw data, and reading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onAqQ1ralxo8"
   },
   "outputs": [],
   "source": [
    "def setup_project_directories():\n",
    "    \"\"\"Create organized directory structure\"\"\"\n",
    "    if IN_COLAB:\n",
    "        base_path = Path('/content/drive/MyDrive/APS360_Team_2_Project')\n",
    "    else:\n",
    "        base_path = Path('./APS360_Team_2_Project')\n",
    "\n",
    "    directories = {\n",
    "        'data_raw': base_path / 'data' / 'raw',\n",
    "        'data_processed': base_path / 'data' / 'processed',\n",
    "        'data_cached': base_path / 'data' / 'cached' / CACHE_SUFFIX,\n",
    "        'models': base_path / 'models',\n",
    "        'logs': base_path / 'logs',\n",
    "        'results': base_path / 'results'\n",
    "    }\n",
    "\n",
    "    for name, path in directories.items():\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Handle cache regeneration\n",
    "    if CACHE_PROCESSED_DATA and REGENERATE_CACHE:\n",
    "        cache_dir = directories['data_cached']\n",
    "        if cache_dir.exists():\n",
    "            import shutil\n",
    "            shutil.rmtree(cache_dir)\n",
    "            cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"Cache directory cleaned\")\n",
    "\n",
    "    print(f\"Project directories created at: {base_path}\")\n",
    "    return directories\n",
    "\n",
    "def get_cache_paths(split_name: str, cache_dir: Path) -> Dict[str, Path]:\n",
    "    \"\"\"Get cache file paths for a specific split\"\"\"\n",
    "    return {\n",
    "        'metadata': cache_dir / f'{split_name}_metadata.json',\n",
    "        'audio': cache_dir / f'{split_name}_audio_features.npy',\n",
    "        'piano_roll': cache_dir / f'{split_name}_piano_roll.npy',\n",
    "        'left_hand': cache_dir / f'{split_name}_left_hand.npy',\n",
    "        'right_hand': cache_dir / f'{split_name}_right_hand.npy',\n",
    "        'config': cache_dir / 'processing_config.json'\n",
    "    }\n",
    "\n",
    "def save_processing_config(cache_dir: Path):\n",
    "    \"\"\"Save processing configuration\"\"\"\n",
    "    config = {\n",
    "        'sample_rate': SAMPLE_RATE,\n",
    "        'n_mels': N_MELS,\n",
    "        'window_size_seconds': WINDOW_SIZE_SECONDS,\n",
    "        'cache_version': CACHE_VERSION,\n",
    "        'n_piano_keys': N_PIANO_KEYS,\n",
    "        'min_midi_note': MIN_MIDI_NOTE,\n",
    "        'datasets_used': DATASETS_TO_USE,\n",
    "        'augmentation_ratio': AUGMENTATION_RATIO,\n",
    "        'mix_volume_range': MIX_VOLUME_RANGE\n",
    "    }\n",
    "\n",
    "    with open(cache_dir / 'processing_config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "def check_cache_compatibility(cache_dir: Path) -> bool:\n",
    "    \"\"\"Check if existing cache is compatible\"\"\"\n",
    "    config_path = cache_dir / 'processing_config.json'\n",
    "    if not config_path.exists():\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            saved_config = json.load(f)\n",
    "\n",
    "        # Check key parameters\n",
    "        return (saved_config.get('cache_version') == CACHE_VERSION and\n",
    "                saved_config.get('sample_rate') == SAMPLE_RATE and\n",
    "                saved_config.get('n_mels') == N_MELS and\n",
    "                saved_config.get('window_size_seconds') == WINDOW_SIZE_SECONDS and\n",
    "                saved_config.get('datasets_used') == DATASETS_TO_USE and\n",
    "                saved_config.get('augmentation_ratio') == AUGMENTATION_RATIO)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def download_maestro_dataset(data_dir: Path) -> Path:\n",
    "    \"\"\"Download MAESTRO dataset if needed\"\"\"\n",
    "    maestro_dir = data_dir / 'maestro-v3.0.0'\n",
    "\n",
    "    if maestro_dir.exists():\n",
    "        print(\"MAESTRO dataset already exists\")\n",
    "        return maestro_dir\n",
    "\n",
    "    print(\"Downloading MAESTRO dataset...\")\n",
    "    maestro_zip = data_dir / 'maestro-v3.0.0.zip'\n",
    "    maestro_url = 'https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0.zip'\n",
    "\n",
    "    if not maestro_zip.exists():\n",
    "        urllib.request.urlretrieve(maestro_url, maestro_zip)\n",
    "        print(\"Download complete\")\n",
    "\n",
    "    print(\"Extracting MAESTRO dataset...\")\n",
    "    with zipfile.ZipFile(maestro_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    print(\"Extraction complete\")\n",
    "\n",
    "    return maestro_dir\n",
    "\n",
    "def find_musdb_dataset(data_dir: Path) -> Path:\n",
    "    \"\"\"Find MUSDB18-HQ dataset in the raw data directory\"\"\"\n",
    "    musdb_dir = data_dir / 'MUSDB18-HQ'\n",
    "\n",
    "    if musdb_dir.exists() and (musdb_dir / 'train').exists():\n",
    "        print(\"MUSDB18-HQ dataset found\")\n",
    "        return musdb_dir\n",
    "\n",
    "    print(f\"MUSDB18-HQ not found, expected location: {musdb_dir}\")\n",
    "    return None\n",
    "\n",
    "def load_maestro_metadata(maestro_path: Path) -> List[Dict]:\n",
    "    \"\"\"Load MAESTRO metadata from CSV\"\"\"\n",
    "    csv_file = maestro_path / 'maestro-v3.0.0.csv'\n",
    "    print(f\"Loading MAESTRO metadata from: {csv_file.name}\")\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[df['duration'] >= MIN_RECORDING_LENGTH]\n",
    "\n",
    "    metadata_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        audio_path = maestro_path / row['audio_filename']\n",
    "        midi_path = maestro_path / row['midi_filename']\n",
    "\n",
    "        if audio_path.exists() and midi_path.exists():\n",
    "            metadata_list.append({\n",
    "                'audio_path': audio_path,\n",
    "                'midi_path': midi_path,\n",
    "                'duration': float(row['duration']),\n",
    "                'title': str(row['canonical_title']),\n",
    "                'composer': str(row['canonical_composer']),\n",
    "                'year': int(row['year']),\n",
    "                'split': str(row['split']),\n",
    "                'dataset': 'maestro'\n",
    "            })\n",
    "\n",
    "    print(f\"Loaded {len(metadata_list)} MAESTRO recordings\")\n",
    "    return metadata_list\n",
    "\n",
    "def load_musdb_stems(musdb_path: Path) -> List[Dict]:\n",
    "    \"\"\"Load MUSDB18-HQ stems from direct file structure for audio augmentation\"\"\"\n",
    "    print(f\"Loading MUSDB18-HQ stems from: {musdb_path.name}\")\n",
    "\n",
    "    train_dir = musdb_path / 'train'\n",
    "    if not train_dir.exists():\n",
    "        print(f\"Training directory not found: {train_dir}\")\n",
    "        return []\n",
    "\n",
    "    stems_list = []\n",
    "\n",
    "    # Scan all track directories in train/\n",
    "    track_folders = [d for d in train_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "    print(f\"Found {len(track_folders)} track folders\")\n",
    "\n",
    "    for track_dir in track_folders:\n",
    "        track_name = track_dir.name\n",
    "\n",
    "        # Each song comes with these stems\n",
    "        required_stems = ['mixture.wav', 'drums.wav', 'bass.wav', 'other.wav', 'vocals.wav']\n",
    "        stem_paths = {}\n",
    "        all_stems_exist = True\n",
    "\n",
    "        for stem_name in required_stems:\n",
    "            stem_path = track_dir / stem_name\n",
    "            if stem_path.exists():\n",
    "                stem_paths[stem_name] = stem_path\n",
    "            else:\n",
    "                print(f\"Missing {stem_name} in {track_name}\")\n",
    "                all_stems_exist = False\n",
    "                break\n",
    "\n",
    "        if all_stems_exist:\n",
    "            # Get duration from mixture file\n",
    "            try:\n",
    "                # Quick duration check without loading full audio\n",
    "                import soundfile as sf\n",
    "                with sf.SoundFile(stem_paths['mixture.wav']) as f:\n",
    "                    duration = len(f) / f.samplerate\n",
    "                    sample_rate = f.samplerate\n",
    "\n",
    "                stems_info = {\n",
    "                    'track_name': track_name,\n",
    "                    'track_dir': track_dir,\n",
    "                    'duration': duration,\n",
    "                    'sample_rate': sample_rate,\n",
    "                    'mixture_path': stem_paths['mixture.wav'],\n",
    "                    'drums_path': stem_paths['drums.wav'],\n",
    "                    'bass_path': stem_paths['bass.wav'],\n",
    "                    'other_path': stem_paths['other.wav'],\n",
    "                    'vocals_path': stem_paths['vocals.wav'],\n",
    "                    'dataset': 'musdb_stems'\n",
    "                }\n",
    "                stems_list.append(stems_info)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {track_name}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(stems_list)} MUSDB tracks for augmentation\")\n",
    "    if stems_list:\n",
    "        avg_duration = np.mean([s['duration'] for s in stems_list])\n",
    "        total_hours = sum([s['duration'] for s in stems_list]) / 3600\n",
    "\n",
    "    return stems_list\n",
    "\n",
    "def load_combined_metadata(data_dir: Path) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Load metadata from MAESTRO and MUSDB stems\"\"\"\n",
    "    maestro_metadata = []\n",
    "    musdb_stems = []\n",
    "\n",
    "    # Load MAESTRO (pure piano)\n",
    "    if DATASETS_TO_USE.get('maestro', False):\n",
    "        print(\"\\nLoading MAESTRO dataset...\")\n",
    "        maestro_dir = download_maestro_dataset(data_dir)\n",
    "        maestro_metadata = load_maestro_metadata(maestro_dir)\n",
    "\n",
    "    # Load MUSDB stems (for augmentation)\n",
    "    if DATASETS_TO_USE.get('musdb_augmentation', False):\n",
    "        print(\"\\nLoading MUSDB18-HQ stems...\")\n",
    "        musdb_dir = find_musdb_dataset(data_dir)\n",
    "        if musdb_dir:\n",
    "            musdb_stems = load_musdb_stems(musdb_dir)\n",
    "        else:\n",
    "            print(\"MUSDB dataset not found, continuing without augmentation\")\n",
    "\n",
    "    print(f\"\\nCombined dataset summary:\")\n",
    "    print(f\"   MAESTRO: {len(maestro_metadata):,} recordings\")\n",
    "    print(f\"   MUSDB stems: {len(musdb_stems):,} tracks\")\n",
    "\n",
    "    return maestro_metadata, musdb_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fyomwCplxo9"
   },
   "source": [
    "## 4. Data Processing Functions\n",
    "This whole section creates all the data filters and any functions we might need for when we start building the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gm7LRMd_lxo9"
   },
   "outputs": [],
   "source": "def standardize_audio(audio_path: Path) -> Tuple[np.ndarray, int]:\n    \"\"\"Standardize audio: resample to 16kHz mono and normalize volume\"\"\"\n    try:\n        # Load and convert to mono\n        audio, sr = librosa.load(audio_path, sr=None, mono=True)\n\n        # Resample if needed\n        if sr != SAMPLE_RATE:\n            audio = librosa.resample(audio, orig_sr=sr, target_sr=SAMPLE_RATE)\n\n        # Normalize volume\n        rms = np.sqrt(np.mean(audio**2))\n        if rms > 1e-6:\n            audio = audio * (0.1 / rms)  # Normalize to target RMS\n            audio = np.clip(audio, -0.95, 0.95)  # Prevent clipping\n\n        return audio, SAMPLE_RATE\n    except Exception as e:\n        print(f\"Audio error {audio_path}: {e}\")\n        return None, None\n\ndef extract_audio_features(audio: np.ndarray, sr: int) -> np.ndarray:\n    \"\"\"Extract mel spectrogram features\"\"\"\n    try:\n        mel_spec = librosa.feature.melspectrogram(\n            y=audio, sr=sr, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH\n        )\n        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n        mel_spec_norm = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n        features = mel_spec_norm.T  # (time, features)\n        \n        # DEBUG: Print feature dimensions\n        print(f\"üîç DEBUG - Audio features shape: {features.shape} (should be [time, 128])\")\n        \n        return features\n    except Exception as e:\n        print(f\"Feature extraction error: {e}\")\n        return None\n\ndef process_midi_to_piano_roll(midi_path: Path) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Convert MIDI to piano roll with hand separation\"\"\"\n    try:\n        midi_data = pretty_midi.PrettyMIDI(str(midi_path))\n        fps = SAMPLE_RATE // HOP_LENGTH\n\n        # Get piano roll and extract piano range\n        piano_roll = midi_data.get_piano_roll(fs=fps)\n        piano_roll = piano_roll[MIN_MIDI_NOTE:MAX_MIDI_NOTE+1, :]\n        piano_roll = (piano_roll > 0).astype(np.float32)\n        piano_roll = piano_roll.T  # (time, keys)\n\n        # DEBUG: Print piano roll dimensions\n        print(f\"üîç DEBUG - Piano roll shape: {piano_roll.shape} (should be [time, 88])\")\n\n        # Hand separation\n        middle_c_index = MIDDLE_C - MIN_MIDI_NOTE\n        left_hand = np.zeros_like(piano_roll)\n        right_hand = np.zeros_like(piano_roll)\n\n        left_hand[:, :middle_c_index] = piano_roll[:, :middle_c_index]\n        right_hand[:, middle_c_index:] = piano_roll[:, middle_c_index:]\n\n        return piano_roll, left_hand, right_hand\n    except Exception as e:\n        print(f\"MIDI error {midi_path}: {e}\")\n        return None, None, None\n\ndef extract_matching_audio_segment(audio_path: Path, start_time: float, duration: float, target_sr: int) -> np.ndarray:\n    \"\"\"Extract exact length segment from MUSDB stems with proper resampling\"\"\"\n    try:\n        # Load segment with exact timing\n        audio, original_sr = librosa.load(\n            audio_path,\n            sr=target_sr,\n            offset=start_time,\n            duration=duration,\n            mono=True\n        )\n\n        # Ensure exact sample count\n        target_samples = int(duration * target_sr)  # 160,000 for 10s at 16kHz\n\n        if len(audio) > target_samples:\n            audio = audio[:target_samples]  # Trim if too long\n        elif len(audio) < target_samples:\n            # Pad with silence if too short\n            padding = target_samples - len(audio)\n            audio = np.pad(audio, (0, padding), mode='constant', constant_values=0.0)\n\n        # Normalize volume\n        rms = np.sqrt(np.mean(audio**2))\n        if rms > 1e-6:\n            audio = audio * (0.1 / rms)\n            audio = np.clip(audio, -0.95, 0.95)\n\n        return audio\n\n    except Exception as e:\n        print(f\"Segment extraction error {audio_path}: {e}\")\n        return np.zeros(int(duration * target_sr))\n\ndef create_mixed_audio(maestro_audio: np.ndarray, musdb_stems_info: Dict) -> np.ndarray:\n    \"\"\"Create mixed audio: MAESTRO piano + MUSDB accompaniment of matching length\"\"\"\n    try:\n        target_sr = SAMPLE_RATE\n        target_duration = len(maestro_audio) / target_sr\n\n        # Choose a safe start time within the MUSDB track\n        track_duration = musdb_stems_info['duration']\n        safe_margin = 2.0\n        max_start = max(0.0, track_duration - target_duration - safe_margin)\n        start_time = random.uniform(0.0, max_start) if max_start > 0 else 0.0\n\n        # Load matching-length stem segments\n        # we only want drum, bass and vocals because others might have piano in it\n        drums = extract_matching_audio_segment(\n            musdb_stems_info['drums_path'], start_time, target_duration, target_sr\n        )\n        bass = extract_matching_audio_segment(\n            musdb_stems_info['bass_path'], start_time, target_duration, target_sr\n        )\n        vocals = extract_matching_audio_segment(\n            musdb_stems_info['vocals_path'], start_time, target_duration, target_sr\n        )\n\n        # Combine accompaniment\n        accompaniment = drums + bass + vocals\n\n        # Ensure exact length match\n        target_samples = len(maestro_audio)\n        if len(accompaniment) > target_samples:\n            accompaniment = accompaniment[:target_samples]\n        elif len(accompaniment) < target_samples:\n            padding = target_samples - len(accompaniment)\n            accompaniment = np.pad(accompaniment, (0, padding), mode='constant', constant_values=0.0)\n\n        # Volume mixing, making sure piano is louder than accompaniment\n        mix_ratio = random.uniform(*MIX_VOLUME_RANGE)\n        mixed_audio = maestro_audio + (accompaniment * mix_ratio)\n        mixed_audio = np.clip(mixed_audio, -0.95, 0.95)\n\n        return mixed_audio\n\n    except Exception as e:\n        print(f\"Audio mixing error: {e}\")\n        return maestro_audio  # Return original if mixing fails\n\ndef extract_sliding_windows(audio_features: np.ndarray, piano_roll: np.ndarray,\n                          left_hand: np.ndarray, right_hand: np.ndarray,\n                          metadata_item: Dict, musdb_stems: List[Dict] = None,\n                          is_training: bool = True) -> List[Dict]:\n    \"\"\"Extract sliding windows with optional MUSDB augmentation\"\"\"\n    min_length = min(len(audio_features), len(piano_roll), len(left_hand), len(right_hand))\n\n    # DEBUG: Print all input dimensions before processing\n    print(f\"üîç DEBUG - extract_sliding_windows input shapes:\")\n    print(f\"   Audio features: {audio_features.shape}\")\n    print(f\"   Piano roll: {piano_roll.shape}\")\n    print(f\"   Left hand: {left_hand.shape}\")\n    print(f\"   Right hand: {right_hand.shape}\")\n    print(f\"   Min length: {min_length}, Window size needed: {WINDOW_SIZE_FRAMES}\")\n\n    if min_length < WINDOW_SIZE_FRAMES:\n        print(f\"‚ùå WARNING: Min length {min_length} < window size {WINDOW_SIZE_FRAMES}, returning empty list\")\n        return []\n\n    # Truncate to same length\n    audio_features = audio_features[:min_length]\n    piano_roll = piano_roll[:min_length]\n    left_hand = left_hand[:min_length]\n    right_hand = right_hand[:min_length]\n\n    # DEBUG: Print truncated shapes\n    print(f\"üîç DEBUG - After truncation:\")\n    print(f\"   Audio features: {audio_features.shape}\")\n    print(f\"   Piano roll: {piano_roll.shape}\")\n\n    windows = []\n\n    if is_training:\n        # Random windows for training\n        max_start = min_length - WINDOW_SIZE_FRAMES\n        num_windows = min(max_start // STRIDE_FRAMES + 1, max(1, min_length // WINDOW_SIZE_FRAMES))\n        start_frames = np.random.randint(0, max_start + 1, size=num_windows)\n    else:\n        # Sequential windows for validation/test\n        start_frames = np.arange(0, min_length - WINDOW_SIZE_FRAMES + 1, STRIDE_FRAMES)\n\n    for i, start_frame in enumerate(start_frames):\n        end_frame = start_frame + WINDOW_SIZE_FRAMES\n\n        # Extract window\n        window_audio = audio_features[start_frame:end_frame].copy()\n        window_piano = piano_roll[start_frame:end_frame].copy()\n        window_left = left_hand[start_frame:end_frame].copy()\n        window_right = right_hand[start_frame:end_frame].copy()\n\n        # DEBUG: Print window shapes for first window\n        if i == 0:\n            print(f\"üîç DEBUG - First window shapes:\")\n            print(f\"   Window audio: {window_audio.shape} (should be [{WINDOW_SIZE_FRAMES}, 128])\")\n            print(f\"   Window piano: {window_piano.shape} (should be [{WINDOW_SIZE_FRAMES}, 88])\")\n\n        # Decide if this window should be augmented\n        use_augmentation = (\n            is_training and\n            musdb_stems and\n            len(musdb_stems) > 0 and\n            random.random() < AUGMENTATION_RATIO\n        )\n\n        if use_augmentation:\n            # Create mixed audio version\n            window_type = 'mixed'\n        else:\n            # Use clean piano version\n            window_type = 'clean'\n\n        window = {\n            'audio': window_audio,\n            'piano_roll': window_piano,\n            'left_hand': window_left,\n            'right_hand': window_right,\n            'metadata': metadata_item,\n            'window_start_time': start_frame * HOP_LENGTH / SAMPLE_RATE,\n            'window_id': f\"{metadata_item['title'][:20]}_{start_frame}\",\n            'window_type': window_type\n        }\n        windows.append(window)\n\n    print(f\"üîç DEBUG - Created {len(windows)} windows from {metadata_item['title'][:30]}\")\n    return windows\n\ndef process_audio_midi_pair(metadata_item: Dict, musdb_stems: List[Dict] = None, is_training: bool = True) -> List[Dict]:\n    \"\"\"Process a single audio-MIDI pair into windows with optional MUSDB augmentation\"\"\"\n    try:\n        print(f\"üîç DEBUG - Processing: {metadata_item['title'][:50]}\")\n        \n        # Process audio\n        audio, sr = standardize_audio(metadata_item['audio_path'])\n        if audio is None:\n            return []\n\n        print(f\"üîç DEBUG - Raw audio length: {len(audio)} samples ({len(audio)/SAMPLE_RATE:.1f}s)\")\n\n        should_augment = (\n            is_training and\n            musdb_stems and\n            len(musdb_stems) > 0 and\n            random.random() < AUGMENTATION_RATIO\n        )\n\n        if should_augment:\n            # Mix with random MUSDB track\n            random_musdb = random.choice(musdb_stems)\n            audio = create_mixed_audio(audio, random_musdb)\n\n        # Extract audio features from final audio\n        audio_features = extract_audio_features(audio, sr)\n        if audio_features is None:\n            return []\n\n        # Process MIDI, we keep the same labels regardless of audio mixing\n        piano_roll, left_hand, right_hand = process_midi_to_piano_roll(metadata_item['midi_path'])\n        if piano_roll is None:\n            return []\n\n        # DEBUG: Check alignment before window extraction\n        print(f\"üîç DEBUG - Pre-window alignment check:\")\n        print(f\"   Audio features: {audio_features.shape[0]} frames\")\n        print(f\"   Piano roll: {piano_roll.shape[0]} frames\") \n        \n        # Calculate expected frames for audio length\n        expected_frames = len(audio) // HOP_LENGTH\n        print(f\"   Expected frames from audio length: {expected_frames}\")\n\n        # Extract windows\n        windows = extract_sliding_windows(\n            audio_features, piano_roll, left_hand, right_hand,\n            metadata_item, musdb_stems, is_training\n        )\n\n        return windows\n\n    except Exception as e:\n        print(f\"Error processing {metadata_item['title']}: {e}\")\n        return []"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WL6UJBRblxo-"
   },
   "source": [
    "## 5. Dataset Creation\n",
    "This is the dataset creation and laoding fucntions, it create splits, Dataset class, and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a2qrbrLlxo_"
   },
   "outputs": [],
   "source": [
    "def create_maestro_dataset_splits(metadata_list: List[Dict]) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Create train/val/test splits for MAESTRO dataset with MUSDB augmentation\"\"\"\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    # Filter to only MAESTRO items\n",
    "    maestro_items = [item for item in metadata_list if item['dataset'] == 'maestro']\n",
    "\n",
    "    if not maestro_items:\n",
    "        raise ValueError(\"No MAESTRO recordings found in metadata\")\n",
    "\n",
    "    splits = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "    # Handle MAESTRO with composer-based splits\n",
    "    print(f\"Splitting MAESTRO by composer...\")\n",
    "    composer_groups = defaultdict(list)\n",
    "    for item in maestro_items:\n",
    "        composer_groups[item['composer']].append(item)\n",
    "\n",
    "    composers = list(composer_groups.keys())\n",
    "    np.random.shuffle(composers)\n",
    "\n",
    "    n_composers = len(composers)\n",
    "    train_end = int(n_composers * TRAIN_RATIO)\n",
    "    val_end = train_end + int(n_composers * VAL_RATIO)\n",
    "\n",
    "    for i, composer in enumerate(composers):\n",
    "        if i < train_end:\n",
    "            split = 'train'\n",
    "        elif i < val_end:\n",
    "            split = 'val'\n",
    "        else:\n",
    "            split = 'test'\n",
    "        splits[split].extend(composer_groups[composer])\n",
    "\n",
    "    # Print split statistics\n",
    "    print(f\"MAESTRO dataset splits:\")\n",
    "    for split_name, split_data in splits.items():\n",
    "        composers = set(item['composer'] for item in split_data)\n",
    "        print(f\"   {split_name.upper()}: {len(split_data)} recordings from {len(composers)} composers\")\n",
    "\n",
    "    return splits\n",
    "\n",
    "def save_windows_to_cache(windows: List[Dict], split_name: str, cache_dir: Path):\n",
    "    \"\"\"Save processed windows to cache\"\"\"\n",
    "    if not windows:\n",
    "        return\n",
    "\n",
    "    cache_paths = get_cache_paths(split_name, cache_dir)\n",
    "    print(f\"Saving {len(windows):,} {split_name} windows to cache...\")\n",
    "\n",
    "    # Stack arrays efficiently\n",
    "    audio_features = np.stack([w['audio'] for w in windows])\n",
    "    piano_rolls = np.stack([w['piano_roll'] for w in windows])\n",
    "    left_hands = np.stack([w['left_hand'] for w in windows])\n",
    "    right_hands = np.stack([w['right_hand'] for w in windows])\n",
    "\n",
    "    # Save arrays\n",
    "    np.save(cache_paths['audio'], audio_features)\n",
    "    np.save(cache_paths['piano_roll'], piano_rolls)\n",
    "    np.save(cache_paths['left_hand'], left_hands)\n",
    "    np.save(cache_paths['right_hand'], right_hands)\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_for_json = []\n",
    "    for w in windows:\n",
    "        metadata = w['metadata'].copy()\n",
    "        metadata['audio_path'] = str(metadata['audio_path'])\n",
    "        metadata['midi_path'] = str(metadata['midi_path'])\n",
    "\n",
    "        window_info = {\n",
    "            'metadata': metadata,\n",
    "            'window_start_time': w['window_start_time'],\n",
    "            'window_id': w['window_id']\n",
    "        }\n",
    "        metadata_for_json.append(window_info)\n",
    "\n",
    "    with open(cache_paths['metadata'], 'w') as f:\n",
    "        json.dump(metadata_for_json, f, indent=2)\n",
    "\n",
    "    print(f\"{split_name} cache saved\")\n",
    "\n",
    "def load_windows_from_cache(split_name: str, cache_dir: Path) -> List[Dict]:\n",
    "    \"\"\"Load windows from cache\"\"\"\n",
    "    cache_paths = get_cache_paths(split_name, cache_dir)\n",
    "\n",
    "    # Check files exist\n",
    "    required_files = ['metadata', 'audio', 'piano_roll', 'left_hand', 'right_hand']\n",
    "    for file_type in required_files:\n",
    "        if not cache_paths[file_type].exists():\n",
    "            return []\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading {split_name} windows from cache...\")\n",
    "\n",
    "        # Load arrays we can either do it by loading into ram or reading from disk\n",
    "\n",
    "        # audio_features = np.load(cache_paths['audio'])\n",
    "        # piano_rolls = np.load(cache_paths['piano_roll'])\n",
    "        # left_hands = np.load(cache_paths['left_hand'])\n",
    "        # right_hands = np.load(cache_paths['right_hand'])\n",
    "\n",
    "        # Load arrays\n",
    "        audio_features = np.load(cache_paths['audio'], mmap_mode='r')\n",
    "        piano_rolls = np.load(cache_paths['piano_roll'], mmap_mode='r')\n",
    "        left_hands = np.load(cache_paths['left_hand'], mmap_mode='r')\n",
    "        right_hands = np.load(cache_paths['right_hand'], mmap_mode='r')\n",
    "\n",
    "        # Load metadata\n",
    "        with open(cache_paths['metadata'], 'r') as f:\n",
    "            metadata_list = json.load(f)\n",
    "\n",
    "        # Reconstruct windows\n",
    "        windows = []\n",
    "        for i, window_info in enumerate(metadata_list):\n",
    "            metadata = window_info['metadata'].copy()\n",
    "            metadata['audio_path'] = Path(metadata['audio_path'])\n",
    "            metadata['midi_path'] = Path(metadata['midi_path'])\n",
    "\n",
    "            window = {\n",
    "                'audio': audio_features[i],\n",
    "                'piano_roll': piano_rolls[i],\n",
    "                'left_hand': left_hands[i],\n",
    "                'right_hand': right_hands[i],\n",
    "                'metadata': metadata,\n",
    "                'window_start_time': window_info['window_start_time'],\n",
    "                'window_id': window_info['window_id']\n",
    "            }\n",
    "            windows.append(window)\n",
    "\n",
    "        print(f\"Loaded {len(windows):,} {split_name} windows from cache\")\n",
    "        return windows\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cache for {split_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "class PianoTranscriptionDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for piano transcription (MAESTRO + MUSDB augmentation)\"\"\"\n",
    "\n",
    "    def __init__(self, metadata_list: List[Dict], split_name: str = 'train', musdb_stems: List[Dict] = None, cache_dir: Path = None):\n",
    "        self.metadata_list = metadata_list\n",
    "        self.split_name = split_name\n",
    "        self.is_training = (split_name == 'train')\n",
    "        self.musdb_stems = musdb_stems or []\n",
    "\n",
    "        print(f\"Creating {split_name} dataset with {len(metadata_list)} MAESTRO recordings...\")\n",
    "\n",
    "        # Try cache first\n",
    "        self.windows = []\n",
    "        if CACHE_PROCESSED_DATA and cache_dir and check_cache_compatibility(cache_dir):\n",
    "            self.windows = load_windows_from_cache(split_name, cache_dir)\n",
    "\n",
    "        if not self.windows:\n",
    "            # Process from scratch\n",
    "            print(f\"Processing {len(metadata_list)} recordings...\")\n",
    "            self._process_from_scratch()\n",
    "\n",
    "            if CACHE_PROCESSED_DATA and cache_dir:\n",
    "                save_windows_to_cache(self.windows, split_name, cache_dir)\n",
    "                save_processing_config(cache_dir)\n",
    "\n",
    "        print(f\"{split_name} dataset: {len(self.windows):,} windows\")\n",
    "\n",
    "    def _process_from_scratch(self):\n",
    "        \"\"\"Process recordings from scratch with error handling\"\"\"\n",
    "        failed_recordings = 0\n",
    "\n",
    "        for metadata_item in tqdm(self.metadata_list, desc=f\"Processing {self.split_name}\"):\n",
    "            try:\n",
    "                windows = process_audio_midi_pair(metadata_item, self.musdb_stems, self.is_training)\n",
    "\n",
    "                if len(windows) == 0:\n",
    "                    failed_recordings += 1\n",
    "                else:\n",
    "                    self.windows.extend(windows)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {metadata_item['title']}: {e}\")\n",
    "                failed_recordings += 1\n",
    "\n",
    "        if failed_recordings > 0:\n",
    "            success_rate = (len(self.metadata_list) - failed_recordings) / len(self.metadata_list) * 100\n",
    "            print(f\"{failed_recordings} recordings failed, {success_rate:.1f}% success rate\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"Get preprocessed window\"\"\"\n",
    "        window = self.windows[idx]\n",
    "\n",
    "        return {\n",
    "            'audio': torch.FloatTensor(window['audio']),\n",
    "            'piano_roll': torch.FloatTensor(window['piano_roll']),\n",
    "            'left_hand': torch.FloatTensor(window['left_hand']),\n",
    "            'right_hand': torch.FloatTensor(window['right_hand']),\n",
    "            'metadata': window['metadata'],\n",
    "            'window_info': {\n",
    "                'window_id': window['window_id'],\n",
    "                'start_time': window['window_start_time']\n",
    "            }\n",
    "        }\n",
    "\n",
    "def collate_batch(batch: List[Dict]) -> Dict:\n",
    "    \"\"\"Collate function for DataLoader\"\"\"\n",
    "    if not batch:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'audio': torch.stack([item['audio'] for item in batch]),\n",
    "        'piano_roll': torch.stack([item['piano_roll'] for item in batch]),\n",
    "        'left_hand': torch.stack([item['left_hand'] for item in batch]),\n",
    "        'right_hand': torch.stack([item['right_hand'] for item in batch]),\n",
    "        'metadata': [item['metadata'] for item in batch],\n",
    "        'window_info': [item['window_info'] for item in batch]\n",
    "    }\n",
    "\n",
    "def create_data_loaders(dataset_splits: Dict[str, List[Dict]], cache_dir: Path = None, musdb_stems: List[Dict] = None) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"Create PyTorch DataLoaders for MAESTRO dataset with MUSDB augmentation\"\"\"\n",
    "    print(f\"Creating datasets...\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}, Workers: {NUM_WORKERS}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = PianoTranscriptionDataset(dataset_splits['train'], split_name='train', musdb_stems=musdb_stems, cache_dir=cache_dir)\n",
    "    val_dataset = PianoTranscriptionDataset(dataset_splits['val'], split_name='val', musdb_stems=musdb_stems, cache_dir=cache_dir)\n",
    "    test_dataset = PianoTranscriptionDataset(dataset_splits['test'], split_name='test', musdb_stems=musdb_stems, cache_dir=cache_dir)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "    )\n",
    "\n",
    "    print(f\"Data loaders created:\")\n",
    "    print(f\"Train:{len(train_loader)} batches ({len(train_dataset):,} windows)\")\n",
    "    print(f\"Val:{len(val_loader)} batches ({len(val_dataset):,} windows)\")\n",
    "    print(f\"Test:{len(test_loader)} batches ({len(test_dataset):,} windows)\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TO5o4vblxo_"
   },
   "source": [
    "## 6. Execute Data Pipeline\n",
    "This runs the complete pipeline and creates everything thats needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hg5ZjcGdlxo_",
    "outputId": "76f721ec-0fcf-4423-cc58-610755224d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directories created at: /content/drive/MyDrive/APS360_Team_2_Project\n",
      "Data loading functions ready\n",
      "\n",
      "Loading MAESTRO and MUSDB datasets\n",
      "\n",
      "Loading MAESTRO dataset...\n",
      "MAESTRO dataset already exists\n",
      "Loading MAESTRO metadata from: maestro-v3.0.0.csv\n",
      "Loaded 1276 MAESTRO recordings\n",
      "\n",
      "Loading MUSDB18-HQ stems...\n",
      "MUSDB18-HQ dataset found\n",
      "Loading MUSDB18-HQ stems from: MUSDB18-HQ\n",
      "Found 100 track folders\n",
      "Loaded 100 MUSDB tracks for augmentation\n",
      "\n",
      "Combined dataset summary:\n",
      "   MAESTRO: 1,276 recordings\n",
      "   MUSDB stems: 100 tracks\n",
      "\n",
      "Creating MAESTRO dataset splits\n",
      "Splitting MAESTRO by composer...\n",
      "MAESTRO dataset splits:\n",
      "   TRAIN: 927 recordings from 42 composers\n",
      "   VAL: 211 recordings from 9 composers\n",
      "   TEST: 138 recordings from 9 composers\n",
      "\n",
      "Creating data loaders\n",
      "Creating datasets...\n",
      "Batch size: 8, Workers: 0\n",
      "Creating train dataset with 927 MAESTRO recordings...\n",
      "Loading train windows from cache...\n",
      "Loaded 49,316 train windows from cache\n",
      "train dataset: 49,316 windows\n",
      "Creating val dataset with 211 MAESTRO recordings...\n",
      "Loading val windows from cache...\n",
      "Loaded 20,981 val windows from cache\n",
      "val dataset: 20,981 windows\n",
      "Creating test dataset with 138 MAESTRO recordings...\n",
      "Loading test windows from cache...\n",
      "Loaded 7,261 test windows from cache\n",
      "test dataset: 7,261 windows\n",
      "Data loaders created:\n",
      "Train:6164 batches (49,316 windows)\n",
      "Val:2623 batches (20,981 windows)\n",
      "Test:908 batches (7,261 windows)\n",
      "\n",
      "\n",
      "Data loaded successfully!\n",
      "\n",
      "Final Summary:\n",
      "MAESTRO: 1,276 recordings\n",
      "MUSDB stems available: 100 tracks for augmentation\n",
      "\n",
      "Variables: train_loader, val_loader, test_loader\n"
     ]
    }
   ],
   "source": [
    "# Initialize directories\n",
    "directories = setup_project_directories()\n",
    "print(\"Data loading functions ready\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\nLoading MAESTRO and MUSDB datasets\")\n",
    "maestro_metadata, musdb_stems = load_combined_metadata(directories['data_raw'])\n",
    "\n",
    "if len(maestro_metadata) == 0:\n",
    "    print(\"No datasets loaded successfully!\")\n",
    "    raise RuntimeError(\"No valid datasets found\")\n",
    "\n",
    "# Create splits\n",
    "print(\"\\nCreating MAESTRO dataset splits\")\n",
    "dataset_splits = create_maestro_dataset_splits(maestro_metadata)\n",
    "\n",
    "# Create data loaders\n",
    "print(\"\\nCreating data loaders\")\n",
    "cache_dir = directories['data_cached'] if CACHE_PROCESSED_DATA else None\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(dataset_splits, cache_dir=cache_dir, musdb_stems=musdb_stems)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Data loaded successfully!\")\n",
    "\n",
    "# Count datasets in results\n",
    "dataset_counts = defaultdict(int)\n",
    "for item in maestro_metadata:\n",
    "    dataset_counts[item['dataset']] += 1\n",
    "\n",
    "print(f\"\\nFinal Summary:\")\n",
    "for dataset, count in dataset_counts.items():\n",
    "    print(f\"{dataset.upper()}: {count:,} recordings\")\n",
    "print(f\"MUSDB stems available: {len(musdb_stems)} tracks for augmentation\")\n",
    "\n",
    "print(f\"\\nVariables: train_loader, val_loader, test_loader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgqVSHjKQST0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CRNN_OnsetsAndFrames(nn.Module):\n",
    "    \"\"\"\n",
    "    Input  (batch):  audio features shaped [B, 128, T]  (128 mel bins x T frames)\n",
    "    Output (batch):  logits shaped      [B, 88,  T]     (88 piano keys x T frames)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - We keep time resolution by pooling only along the frequency axis (2,1).\n",
    "    - We return *logits* (NO sigmoid) so that BCEWithLogitsLoss can be used correctly.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pitches: int = 88, lstm_hidden_size: int = 256, cnn_out_channels: int = 128):\n",
    "        super().__init__()\n",
    "        self.name = \"crnn_onsets_frames\"\n",
    "        self.num_pitches = num_pitches\n",
    "\n",
    "        # Convolutional feature extractor (freq pooling only)\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3,3), padding=(1,1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),  # 128 -> 64\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=(3,3), padding=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),  # 64 -> 32\n",
    "\n",
    "            nn.Conv2d(64, cnn_out_channels, kernel_size=(3,3), padding=(1,1)),\n",
    "            nn.BatchNorm2d(cnn_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),  # 32 -> 16\n",
    "        )\n",
    "\n",
    "        # After 3x (2,1) pools, frequency dim: 128 -> 64 -> 32 -> 16\n",
    "        freq_out = 128 // 8  # = 16\n",
    "        lstm_input_size = cnn_out_channels * freq_out\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=False,\n",
    "        )\n",
    "\n",
    "        # Residual projection to match BiLSTM output size (2 * hidden)\n",
    "        self.res_fc = nn.Linear(lstm_input_size, 2 * lstm_hidden_size)\n",
    "\n",
    "        # Frame-wise classifier to 88 piano keys\n",
    "        self.fc_frame = nn.Linear(2 * lstm_hidden_size, num_pitches)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, 128, T]  -> returns logits [B, 88, T]\n",
    "        \"\"\"\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Expected input [B, 128, T], got {tuple(x.shape)}\")\n",
    "\n",
    "        # Add channel dim for convs: [B, 1, 128, T]\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # [B, C, F', T] with F' = 16\n",
    "        x = self.conv_block(x)\n",
    "        B, C, Freq, T = x.shape\n",
    "\n",
    "        # Prepare sequence for LSTM: [T, B, C*F']\n",
    "        x_seq = x.permute(3, 0, 1, 2).contiguous().view(T, B, C * Freq)\n",
    "\n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.lstm(x_seq)  # [T, B, 2*hidden]\n",
    "\n",
    "        # Residual connection (project x_seq to BiLSTM size and add)\n",
    "        lstm_out = lstm_out + self.res_fc(x_seq)\n",
    "\n",
    "        # Frame-wise logits -> [T, B, 88] then -> [B, 88, T]\n",
    "        frame_logits = self.fc_frame(lstm_out)\n",
    "        return frame_logits.permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHra4C9WQV39"
   },
   "outputs": [],
   "source": "import numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef get_model_name(name, batch_size, learning_rate, epoch):\n    return f\"model_{name}_bs{batch_size}_lr{learning_rate}_epoch{epoch}\"\n\ndef evaluate(model, data_loader, criterion):\n    device = next(model.parameters()).device\n    model.eval()\n    total_loss = 0.0\n    total_incorrect = 0\n    total_elements = 0\n    batch_count = 0\n\n    with torch.no_grad():\n        for batch in data_loader:\n            # Expect a dict from collate_fn\n            audio = batch[\"audio\"].to(device)            # [B, T, 128]\n            target = batch[\"piano_roll\"].to(device)      # [B, T, 88]\n\n            # DEBUG: Print shapes before processing\n            print(f\"üîç DEBUG - Input shapes: audio={audio.shape}, target={target.shape}\")\n\n            # Transpose audio for model: [B, T, 128] -> [B, 128, T]\n            audio = audio.transpose(1, 2)\n\n            # Align target to logits shape [B, 88, T]\n            target = target.permute(0, 2, 1).contiguous()\n\n            print(f\"üîç DEBUG - After transpose: audio={audio.shape}, target={target.shape}\")\n\n            logits = model(audio)                        # [B, 88, T]\n            print(f\"üîç DEBUG - Model output: {logits.shape}\")\n\n            # Ensure shapes match before loss calculation\n            if logits.shape != target.shape:\n                print(f\"‚ùå SHAPE MISMATCH: logits={logits.shape}, target={target.shape}\")\n                # Try to fix by truncating to min size\n                min_time = min(logits.shape[2], target.shape[2])\n                logits = logits[:, :, :min_time]\n                target = target[:, :, :min_time]\n                print(f\"üîß FIXED: logits={logits.shape}, target={target.shape}\")\n\n            loss = criterion(logits, target)\n            total_loss += loss.item()\n            batch_count += 1\n\n            # Metrics: thresholded predictions vs targets\n            probs = torch.sigmoid(logits)\n            preds = (probs > 0.5)\n            incorrect = (preds != target.bool())\n            total_incorrect += incorrect.sum().item()\n            total_elements += target.numel()\n\n    error = total_incorrect / max(1, total_elements)\n    avg_loss = total_loss / max(1, batch_count)\n    return avg_loss, error\n\ndef plot_training_curves(save_path, num_epochs):\n    # Load metrics\n    train_loss = np.loadtxt(f\"{save_path}_train_loss.csv\")\n    train_err  = np.loadtxt(f\"{save_path}_train_err.csv\")\n    val_loss   = np.loadtxt(f\"{save_path}_val_loss.csv\")\n    val_err    = np.loadtxt(f\"{save_path}_val_err.csv\")\n\n    epochs = np.arange(1, num_epochs + 1)\n\n    # Loss curve\n    plt.figure()\n    plt.plot(epochs, train_loss, label=\"Training\")\n    plt.plot(epochs, val_loss, label=\"Validation\")\n    plt.title(\"Training and Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    # Error curve\n    plt.figure()\n    plt.plot(epochs, train_err, label=\"Training\")\n    plt.plot(epochs, val_err, label=\"Validation\")\n    plt.title(\"Training and Validation Error\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Error\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef train(model, train_loader, val_loader, batch_size=4, learning_rate=1e-3, num_epochs=30, pos_weight_value=5.0):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n\n    # BCEWithLogitsLoss expects LOGITS; model.forward returns logits.\n    # Use a per-class pos_weight to handle class imbalance across 88 keys.\n    pos_weight = torch.full((model.num_pitches,), float(pos_weight_value), device=device)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Tracking\n    train_losses = np.zeros(num_epochs, dtype=np.float32)\n    train_errs   = np.zeros(num_epochs, dtype=np.float32)\n    val_losses   = np.zeros(num_epochs, dtype=np.float32)\n    val_errs     = np.zeros(num_epochs, dtype=np.float32)\n\n    start_time = time.time()\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        running_incorrect = 0\n        running_total = 0\n        batch_count = 0\n\n        for batch_idx, batch in enumerate(train_loader):\n            audio = batch[\"audio\"].to(device)           # [B, T, 128]\n            target = batch[\"piano_roll\"].to(device)     # [B, T, 88]\n\n            # DEBUG: Print shapes for first batch of first epoch\n            if epoch == 0 and batch_idx == 0:\n                print(f\"üîç DEBUG - Training batch shapes: audio={audio.shape}, target={target.shape}\")\n\n            # Transpose for model: [B, T, 128] -> [B, 128, T]\n            audio = audio.transpose(1, 2)\n            target = target.permute(0, 2, 1).contiguous()  # -> [B, 88, T]\n\n            if epoch == 0 and batch_idx == 0:\n                print(f\"üîç DEBUG - After transpose: audio={audio.shape}, target={target.shape}\")\n\n            optimizer.zero_grad()\n            logits = model(audio)                        # [B, 88, T]\n\n            if epoch == 0 and batch_idx == 0:\n                print(f\"üîç DEBUG - Model output: {logits.shape}\")\n\n            # Ensure shapes match before loss calculation\n            if logits.shape != target.shape:\n                print(f\"‚ùå SHAPE MISMATCH in batch {batch_idx}: logits={logits.shape}, target={target.shape}\")\n                # Fix by truncating to min size\n                min_time = min(logits.shape[2], target.shape[2])\n                logits = logits[:, :, :min_time]\n                target = target[:, :, :min_time]\n                print(f\"üîß FIXED to: logits={logits.shape}, target={target.shape}\")\n\n            loss = criterion(logits, target.float())\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            batch_count += 1\n\n            with torch.no_grad():\n                probs = torch.sigmoid(logits)\n                preds = (probs > 0.5)\n                incorrect = (preds != target.bool())\n                running_incorrect += incorrect.sum().item()\n                running_total += target.numel()\n\n        # Epoch metrics\n        train_losses[epoch] = running_loss / max(1, batch_count)\n        train_errs[epoch]   = (running_incorrect / max(1, running_total))\n\n        # Validation\n        val_loss, val_err = evaluate(model, val_loader, criterion)\n        val_losses[epoch] = val_loss\n        val_errs[epoch]   = val_err\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n              f\"Train Loss: {train_losses[epoch]:.4f}  Err: {train_errs[epoch]:.4f} | \"\n              f\"Val Loss: {val_losses[epoch]:.4f}  Err: {val_errs[epoch]:.4f}\")\n\n        # Save model checkpoint each epoch\n        model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n        torch.save(model.state_dict(), model_path)\n\n    elapsed = time.time() - start_time\n    print(f\"\\nTraining completed in {elapsed:.1f}s\")\n\n    # Save metrics and plot\n    save_path = f\"model_{model.name}_bs{batch_size}_lr{learning_rate}\"\n    np.savetxt(f\"{save_path}_train_loss.csv\", train_losses)\n    np.savetxt(f\"{save_path}_train_err.csv\",  train_errs)\n    np.savetxt(f\"{save_path}_val_loss.csv\",   val_losses)\n    np.savetxt(f\"{save_path}_val_err.csv\",    val_errs)\n\n    plot_training_curves(save_path, num_epochs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "LjjOaXilQpzT",
    "outputId": "029832d0-dec3-41e2-e422-aa56b415bba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset with 927 MAESTRO recordings...\n",
      "Loading train windows from cache...\n",
      "Loaded 49,316 train windows from cache\n",
      "train dataset: 49,316 windows\n",
      "Overfit subset size: 64\n",
      "Batch size set to:   64 (1 step per epoch)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pathlib.PosixPath'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[0;32m--> 171\u001b[0;31m                     {\n\u001b[0m\u001b[1;32m    172\u001b[0m                         key: collate(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pathlib.PosixPath'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[0;32m--> 171\u001b[0;31m                     {\n\u001b[0m\u001b[1;32m    172\u001b[0m                         key: collate(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             return {\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    191\u001b[0m             return {\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pathlib.PosixPath'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[0;32m--> 171\u001b[0;31m                     {\n\u001b[0m\u001b[1;32m    172\u001b[0m                         key: collate(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pathlib.PosixPath'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-820776207.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m train(\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0moverfit_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4236819120.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, val_loader, batch_size, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbatch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;31m# The mapping type may not support `copy()` / `update(mapping)`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             return {\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             return {\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             }\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;31m# The mapping type may not support `copy()` / `update(mapping)`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             return {\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# or `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             return {\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             }\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 ]\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pathlib.PosixPath'>"
     ]
    }
   ],
   "source": [
    "# --- Overfit-on-a-tiny-slice demo ---\n",
    "# This cell creates a very small, deterministic subset of the training set,\n",
    "# sets the batch size equal to that subset (single-batch epochs),\n",
    "# and trains long enough to intentionally overfit. A training curve is plotted at the end.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Reproducibility & deterministic ops\n",
    "torch.manual_seed(42)\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: SETUP OVERFIT DATASET\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: SETTING UP OVERFIT DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build the training dataset\n",
    "train_dataset = PianoTranscriptionDataset(\n",
    "    dataset_splits['train'],\n",
    "    split_name='train',\n",
    "    musdb_stems=musdb_stems,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# Pick a tiny, contiguous slice from the start for stability\n",
    "OVERFIT_FRACTION = 0.05     # 5% of train\n",
    "MIN_EXAMPLES = 8            # ensure we have enough examples to learn\n",
    "MAX_EXAMPLES = 16           # Reduced for memory - was 64\n",
    "overfit_size = max(MIN_EXAMPLES, int(len(train_dataset) * OVERFIT_FRACTION))\n",
    "overfit_size = min(overfit_size, MAX_EXAMPLES, len(train_dataset))\n",
    "\n",
    "overfit_indices = list(range(overfit_size))\n",
    "overfit_subset = Subset(train_dataset, overfit_indices)\n",
    "\n",
    "# Use smaller batch size to prevent memory crashes\n",
    "BATCH_SIZE = 4  # Much smaller batch size for CPU\n",
    "overfit_loader = DataLoader(\n",
    "    overfit_subset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,      # deterministic order\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_batch  # Use the custom collate function\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset: {len(train_dataset):,} total windows\")\n",
    "print(f\"‚úÖ Overfit subset size: {len(overfit_subset)}\")\n",
    "print(f\"‚úÖ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"‚úÖ DataLoader using custom collate: {overfit_loader.collate_fn == collate_batch}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# STEP 2: VERIFY DATALOADER AND SHAPES\n# =============================================================================\nprint(\"=\"*80)\nprint(\"STEP 2: VERIFYING DATALOADER AND TENSOR SHAPES\")\nprint(\"=\"*80)\n\n# Step 2.1: Test DataLoader\nprint(\"2.1 Testing DataLoader...\")\ntry:\n    test_batch = next(iter(overfit_loader))\n    print(f\"‚úÖ DataLoader works! Batch keys: {test_batch.keys()}\")\n    \n    # DEBUG: Print actual shapes\n    audio_shape = test_batch['audio'].shape\n    piano_shape = test_batch['piano_roll'].shape\n    print(f\"üîç DEBUG - Audio shape from DataLoader: {audio_shape}\")\n    print(f\"üîç DEBUG - Piano roll shape from DataLoader: {piano_shape}\")\n    print(f\"üîç DEBUG - Expected: Audio [4, 312, 128], Piano [4, 312, 88]\")\n    \n    # Check if shapes match in time dimension\n    if audio_shape[1] != piano_shape[1]:\n        print(f\"‚ùå DIMENSION MISMATCH: Audio time={audio_shape[1]}, Piano time={piano_shape[1]}\")\n        print(f\"‚ùå This is likely the root cause of the tensor size error!\")\n    else:\n        print(f\"‚úÖ Time dimensions match: {audio_shape[1]}\")\n        \nexcept Exception as e:\n    print(f\"‚ùå DataLoader failed: {e}\")\n    print(\"STOP: DataLoader issue - check collate function\")\n    raise\n\n# Step 2.2: Test model with corrected input\nprint(\"\\n2.2 Testing model with corrected input...\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = CRNN_OnsetsAndFrames().to(device)\n\n# Apply the transpose fixes\ntest_audio = test_batch[\"audio\"].to(device)           # [B, T, 128]\ntest_target = test_batch[\"piano_roll\"].to(device)     # [B, T, 88]\n\nprint(f\"üîç DEBUG - Before transpose - Audio: {test_audio.shape}, Target: {test_target.shape}\")\n\n# CRITICAL FIX: Transpose to match model expectation\ntest_audio_transposed = test_audio.transpose(1, 2)    # [B, 128, T]\ntest_target_transposed = test_target.permute(0, 2, 1).contiguous()  # [B, 88, T]\n\nprint(f\"üîç DEBUG - After transpose - Audio: {test_audio_transposed.shape}, Target: {test_target_transposed.shape}\")\nprint(f\"‚úÖ Expected: Audio [4, 128, 312], Target [4, 88, 312]\")\n\n# Check if transposed shapes have matching time dimension\nif test_audio_transposed.shape[2] != test_target_transposed.shape[2]:\n    print(f\"‚ùå TRANSPOSED DIMENSION MISMATCH: Audio time={test_audio_transposed.shape[2]}, Target time={test_target_transposed.shape[2]}\")\nelse:\n    print(f\"‚úÖ Transposed time dimensions match: {test_audio_transposed.shape[2]}\")\n\n# Step 2.3: Test model forward pass\nprint(\"\\n2.3 Testing model forward pass...\")\ntry:\n    with torch.no_grad():\n        test_logits = model(test_audio_transposed)\n        print(f\"üîç DEBUG - Model output shape: {test_logits.shape}\")\n        print(f\"üîç DEBUG - Expected output shape: [4, 88, 312]\")\n        \n        # Check if model output matches target shape\n        if test_logits.shape != test_target_transposed.shape:\n            print(f\"‚ùå MODEL OUTPUT MISMATCH: Model={test_logits.shape}, Target={test_target_transposed.shape}\")\n        else:\n            print(f\"‚úÖ Model output matches target shape!\")\n            \nexcept Exception as e:\n    print(f\"‚ùå Model forward pass failed: {e}\")\n    print(\"STOP: Model architecture issue\")\n    raise\n\n# Step 2.4: Test loss calculation\nprint(\"\\n2.4 Testing loss calculation...\")\ntry:\n    criterion = torch.nn.BCEWithLogitsLoss()\n    test_loss = criterion(test_logits, test_target_transposed.float())\n    print(f\"‚úÖ Loss calculation works! Loss: {test_loss.item():.4f}\")\nexcept Exception as e:\n    print(f\"‚ùå Loss calculation failed: {e}\")\n    print(f\"üîç DEBUG - Logits shape: {test_logits.shape}\")\n    print(f\"üîç DEBUG - Target shape: {test_target_transposed.shape}\")\n    print(\"STOP: Loss calculation issue\")\n    raise\n\nprint(\"\\n‚úÖ ALL VERIFICATION TESTS PASSED!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# STEP 3: RUN TRAINING WITH DIMENSION FIX\n# =============================================================================\nprint(\"=\"*80)\nprint(\"STEP 3: STARTING TRAINING WITH DIMENSION FIX\")\nprint(\"=\"*80)\n\n# Training parameters\nNUM_EPOCHS = 5  # Short training for testing\nLR = 1e-3\n\nprint(f\"Training parameters:\")\nprint(f\"  ‚Ä¢ Epochs: {NUM_EPOCHS}\")\nprint(f\"  ‚Ä¢ Learning rate: {LR}\")\nprint(f\"  ‚Ä¢ Batch size: {BATCH_SIZE}\")\nprint(f\"  ‚Ä¢ Device: {device}\")\nprint(\"=\"*80)\n\n# CUSTOM TRAINING FUNCTION WITH DIMENSION FIX\ndef train_with_fix(model, train_loader, val_loader, num_epochs=5):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0.0\n        batch_count = 0\n        \n        for batch_idx, batch in enumerate(train_loader):\n            audio = batch[\"audio\"].to(device)           # [B, T, 128]\n            target = batch[\"piano_roll\"].to(device)     # [B, T, 88]\n            \n            # Print shapes for first batch\n            if epoch == 0 and batch_idx == 0:\n                print(f\"üîç Original shapes: audio={audio.shape}, target={target.shape}\")\n            \n            # Transpose to model format\n            audio = audio.transpose(1, 2)               # [B, 128, T]  \n            target = target.permute(0, 2, 1).contiguous()  # [B, 88, T]\n            \n            if epoch == 0 and batch_idx == 0:\n                print(f\"üîç After transpose: audio={audio.shape}, target={target.shape}\")\n            \n            optimizer.zero_grad()\n            logits = model(audio)                       # [B, 88, T]\n            \n            if epoch == 0 and batch_idx == 0:\n                print(f\"üîç Model output: {logits.shape}\")\n            \n            # CRITICAL FIX: Ensure matching time dimensions\n            min_time = min(logits.shape[2], target.shape[2])\n            if logits.shape[2] != target.shape[2]:\n                print(f\"üîß Fixing dimension mismatch: {logits.shape[2]} vs {target.shape[2]} ‚Üí {min_time}\")\n                \n            logits = logits[:, :, :min_time]\n            target = target[:, :, :min_time]\n            \n            if epoch == 0 and batch_idx == 0:\n                print(f\"üîç Final shapes: logits={logits.shape}, target={target.shape}\")\n            \n            loss = criterion(logits, target.float())\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            batch_count += 1\n            \n            # Print progress every 100 batches\n            if batch_idx % 100 == 0:\n                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n        \n        avg_loss = total_loss / max(1, batch_count)\n        print(f\"Epoch {epoch+1}/{num_epochs} completed - Average Loss: {avg_loss:.4f}\")\n    \n    print(\"üéâ Training completed successfully!\")\n\n# Start training with the fixed function\ntrain_with_fix(model, overfit_loader, overfit_loader, num_epochs=NUM_EPOCHS)\n\nprint(\"=\"*80)\nprint(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# STEP 4: SAVE THE TRAINED MODEL FOR WEBSITE INTEGRATION\n# =============================================================================\nprint(\"=\"*80)\nprint(\"STEP 4: SAVING MODEL FOR WEBSITE\")\nprint(\"=\"*80)\n\n# Save the model automatically after training\ntry:\n    print(\"üíæ Saving model for website integration...\")\n    \n    # Use the save function we defined earlier\n    success = save_model_for_website(model, \"piano_transcription_overfitted.pth\")\n    \n    if success:\n        print(\"‚úÖ Model saved successfully!\")\n        print(\"\")\n        print(\"üöÄ NEXT STEPS:\")\n        print(\"1. Download 'piano_transcription_overfitted.pth' from Colab Files\")\n        print(\"2. Place it in your website's backend/weights/ directory\")  \n        print(\"3. Rename it to 'piano_transcription_weights.pth'\")\n        print(\"4. Restart your Flask server\")\n        print(\"5. Test with a piano audio file!\")\n        print(\"\")\n        print(\"üìù Note: This model is overfitted on a small dataset for testing.\")\n        print(\"   For production, train on the full dataset with more epochs.\")\n    else:\n        print(\"‚ùå Model saving failed!\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error saving model: {e}\")\n\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIONAL: QUICK VERIFICATION CHECK  \n",
    "# =============================================================================\n",
    "# Run this cell if you want to double-check that everything is set up correctly\n",
    "# You can skip this and go straight to the training cells above\n",
    "\n",
    "import time\n",
    "print(\"=\"*80)\n",
    "print(\"OPTIONAL VERIFICATION CHECK\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"File has all fixes: BATCH_SIZE=4, transpose fixes, custom collate\")\n",
    "print(f\"Settings: BATCH_SIZE={BATCH_SIZE}, MAX_EXAMPLES={MAX_EXAMPLES}\")\n",
    "print(f\"Overfit subset: {len(overfit_subset)} samples\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}